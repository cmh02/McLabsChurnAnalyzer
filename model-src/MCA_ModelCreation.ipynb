{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281761f9",
   "metadata": {},
   "source": [
    "# MCLabs Churn Analyzer - Model Creation\n",
    "\n",
    "This Jupyter Notebook will create a ML model, train it on our training data, then offer a simple test analysis using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c91861cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODULE/PACKAGE IMPORTS\n",
    "'''\n",
    "\n",
    "# System\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Pipelining\n",
    "import joblib\n",
    "\n",
    "# Output/Display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77fe80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PIPELINE CREATION\n",
    "\n",
    "This section will create a pipeline for loading the data, splitting the data, scaling the data, and training the model.\n",
    "'''\n",
    "\n",
    "# Load the data\n",
    "modelInputDataframe = pd.read_csv(\"../data/targetted/public/1756184400/targetted.csv\")\n",
    "\n",
    "# Separate features from target\n",
    "MCA_Features = modelInputDataframe.drop(columns=[\"churn\"])\n",
    "MCA_Target = modelInputDataframe[\"churn\"]\n",
    "\n",
    "# Split the data\n",
    "MCA_Features_Train, MCA_Features_Test, MCA_Target_Train, MCA_Target_Test = train_test_split(MCA_Features, MCA_Target, test_size=0.2)\n",
    "\n",
    "# Identify which features are categorical\n",
    "categoricalFeatures = [\" plan_player_favorite_server\"]\n",
    "\n",
    "# Identify which features are numerical (note we do not include the last seen time here)\n",
    "numericalFeatures = [\" mcmmo_power_level\", \" mcmmo_skill_ACROBATICS\", \" mcmmo_skill_ALCHEMY\", \" mcmmo_skill_ARCHERY\", \" mcmmo_skill_AXES\", \" mcmmo_skill_CROSSBOWS\", \" mcmmo_skill_EXCAVATION\", \" mcmmo_skill_FISHING\", \" mcmmo_skill_HERBALISM\", \" mcmmo_skill_MACES\", \" mcmmo_skill_MINING\", \" mcmmo_skill_REPAIR\", \" mcmmo_skill_SALVAGE\", \" mcmmo_skill_SMELTING\", \" mcmmo_skill_SWORDS\", \" mcmmo_skill_TAMING\", \" mcmmo_skill_TRIDENTS\", \" mcmmo_skill_UNARMED\", \" mcmmo_skill_WOODCUTTING\", \" lw_rev_total\", \" lw_rev_phase\", \" chemrank\", \" policerank\", \" donorrank\", \" goldrank\", \" current_month_votes\", \" plan_player_time_total_raw\", \" plan_player_time_month_raw\", \" plan_player_time_week_raw\", \" plan_player_time_day_raw\", \" plan_player_time_afk_raw\", \" plan_player_latest_session_length_raw\", \" plan_player_sessions_count\", \" leaderboard_position_chems_all\", \" leaderboard_position_chems_week\", \" leaderboard_position_police_all\", \" leaderboard_position_police_week\", \" balance\", \" plan_player_relativePlaytime_totalmonth\", \" plan_player_relativePlaytime_weekmonth\", \" plan_player_relativePlaytime_dayweek\"]\n",
    "\n",
    "# Create preprocessing transformers for encoding and scaling features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categoricalFeatures),\n",
    "        (\"num\", StandardScaler(), numericalFeatures)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define LogReg pipeline\n",
    "MCA_Pipeline_LogReg = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define XGBoost pipeline\n",
    "MCA_Pipeline_XGB = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", verbosity=0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b4c813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9376053962900506\n",
      "Confusion Matrix:\n",
      "[[ 58  33]\n",
      " [  4 498]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.64      0.76        91\n",
      "           1       0.94      0.99      0.96       502\n",
      "\n",
      "    accuracy                           0.94       593\n",
      "   macro avg       0.94      0.81      0.86       593\n",
      "weighted avg       0.94      0.94      0.93       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LOGREG MODEL TRAINING AND TESTING\n",
    "'''\n",
    "# Train and test LogReg pipeline\n",
    "MCA_Pipeline_LogReg.fit(MCA_Features_Train, MCA_Target_Train)\n",
    "MCA_Target_Pred = MCA_Pipeline_LogReg.predict(MCA_Features_Test)\n",
    "print(f\"Accuracy: {accuracy_score(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(MCA_Target_Test, MCA_Target_Pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79c379aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 400, 'model__scale_pos_weight': 1, 'model__subsample': 1.0}\n",
      "Best Cross-Validation Score: 0.9375776149233843\n",
      "Accuracy: 0.9494097807757167\n",
      "Confusion Matrix:\n",
      "[[ 62  29]\n",
      " [  1 501]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.81        91\n",
      "           1       0.95      1.00      0.97       502\n",
      "\n",
      "    accuracy                           0.95       593\n",
      "   macro avg       0.96      0.84      0.89       593\n",
      "weighted avg       0.95      0.95      0.95       593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "XGBOOST MODEL TRAINING AND TESTING\n",
    "\n",
    "This cell will use a XGBoost pipeline and implement auto hyperparameter tuning to optimize the model's performance.\n",
    "'''\n",
    "\n",
    "# Map of hyperparameters and possible values to try tuning XGBoost with\n",
    "hyperParameterMap = {\n",
    "    \"model__n_estimators\": [100, 200, 400],      # boosting rounds\n",
    "    \"model__max_depth\": [3, 5, 7],               # tree depth\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.3],    # step size shrinkage\n",
    "    \"model__subsample\": [0.8, 1.0],              # row sampling\n",
    "    \"model__colsample_bytree\": [0.8, 1.0],       # feature sampling\n",
    "    \"model__scale_pos_weight\": [1, 2, 5]         # helps with class imbalance\n",
    "}\n",
    "\n",
    "# Grid search for best hyperparameters (5-fold CV)\n",
    "MCA_Pipeline_GridSearch_XGB = GridSearchCV(\n",
    "    estimator=MCA_Pipeline_XGB,\n",
    "    param_grid=hyperParameterMap,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train and test XGBoost pipeline\n",
    "MCA_Pipeline_GridSearch_XGB.fit(MCA_Features_Train, MCA_Target_Train)\n",
    "MCA_Target_Pred = MCA_Pipeline_GridSearch_XGB.predict(MCA_Features_Test)\n",
    "print(f\"Best Parameters: {MCA_Pipeline_GridSearch_XGB.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {MCA_Pipeline_GridSearch_XGB.best_score_}\")\n",
    "print(f\"Accuracy: {accuracy_score(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(MCA_Target_Test, MCA_Target_Pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PIPELINE SAVING\n",
    "\n",
    "This section saves the entire machine learning pipeline to a file for future use.\n",
    "'''\n",
    "\n",
    "# Save the entire LogReg pipeline\n",
    "joblib.dump(MCA_Pipeline_LogReg, \"../model-internals/MCA_Pipeline_LogReg.pkl\")\n",
    "\n",
    "# Save the entire XGBoost pipeline\n",
    "joblib.dump(MCA_Pipeline_GridSearch_XGB, \"../model-internals/MCA_Pipeline_GridSearch_XGB.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
