{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281761f9",
   "metadata": {},
   "source": [
    "# MCLabs Churn Analyzer - Model Creation\n",
    "\n",
    "This Jupyter Notebook will create a ML model, train it on our training data, then offer a simple test analysis using test data.\n",
    "\n",
    "Note that this notebook converts the previous target encoding to a new encoding:\n",
    "- Not Active (Previously 0) -> Dropped\n",
    "- Recovered (Previously 1) -> 0\n",
    "- Churned (Previously 2) -> 1\n",
    "- Active (Previously 3) -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91861cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODULE/PACKAGE IMPORTS\n",
    "'''\n",
    "\n",
    "# System\n",
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Custom Modules\n",
    "from mcalib import McaDataUtils, McaDataPrepare, McaFeaturePipeline, McaTargetPipeline\n",
    "\n",
    "# Pipelining\n",
    "import joblib\n",
    "\n",
    "# Output/Display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12bec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PRE-MODEL DATA PIPELINE\n",
    "'''\n",
    "\n",
    "# Load three data files for model training\n",
    "df_t1 = McaDataUtils.getDfForTimestamp(timestamp=\"1758949983.598\")\n",
    "df_t2 = McaDataUtils.getDfForTimestamp(timestamp=\"1759376248.846\")\n",
    "df_t3 = McaDataUtils.getDfForTimestamp(timestamp=\"1759635486.595\")\n",
    "\n",
    "# Prepare all datasets\n",
    "df_t1 = McaDataPrepare.prepareData(df=df_t1, dfTimestamp=1758949983.598)\n",
    "df_t2 = McaDataPrepare.prepareData(df=df_t2, dfTimestamp=1759376248.846)\n",
    "df_t3 = McaDataPrepare.prepareData(df=df_t3, dfTimestamp=1759635486.595)\n",
    "\n",
    "# Perform feature engineering between the first two timestamps\n",
    "df = McaFeaturePipeline.combineData(currentDf=df_t2, previousDf=df_t1)\n",
    "\n",
    "# Perform target engineering between the last two timestamps\n",
    "df = McaTargetPipeline.buildTarget(currentDf=df, futureDf=df_t3, onlyReturnTarget=False)\n",
    "\n",
    "# Drop UUID's before model\n",
    "df = McaDataUtils.clearUUIDs(df=df)\n",
    "\n",
    "# For now, drop rows where target is 0 (completely inactive)\n",
    "df = df[df[\"churn\"] != 0].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fe80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PIPELINE CREATION\n",
    "\n",
    "This section will create a pipeline for loading the data, splitting the data, scaling the data, and training the model.\n",
    "'''\n",
    "\n",
    "# Separate features from target\n",
    "MCA_Features = df.drop(columns=[\"churn\"])\n",
    "MCA_Target = df[\"churn\"]\n",
    "\n",
    "# Transform target to be 0, 1, 2 instead of 1, 2, 3\n",
    "MCA_LabelEncoder = LabelEncoder()\n",
    "MCA_Target =  MCA_LabelEncoder.fit_transform(MCA_Target)\n",
    "\n",
    "# Split the data\n",
    "MCA_Features_Train, MCA_Features_Test, MCA_Target_Train, MCA_Target_Test = train_test_split(MCA_Features, MCA_Target, test_size=0.2)\n",
    "\n",
    "# Identify which features are categorical\n",
    "categoricalFeatures = [\"plan_player_favorite_server\"]\n",
    "\n",
    "# Identify which features are numerical (note we do not include the last seen time here)\n",
    "numericalFeatures = [\"balance\",\"lw_rev_total\",\"lw_rev_phase\",\"leaderboard_position_chems_all\",\"leaderboard_position_chems_week\",\"leaderboard_position_police_all\",\"leaderboard_position_police_week\",\"mcmmo_power_level\",\"mcmmo_skill_ACROBATICS\",\"mcmmo_skill_ALCHEMY\",\"mcmmo_skill_ARCHERY\",\"mcmmo_skill_AXES\",\"mcmmo_skill_CROSSBOWS\",\"mcmmo_skill_EXCAVATION\",\"mcmmo_skill_FISHING\",\"mcmmo_skill_HERBALISM\",\"mcmmo_skill_MACES\",\"mcmmo_skill_MINING\",\"mcmmo_skill_REPAIR\",\"mcmmo_skill_SALVAGE\",\"mcmmo_skill_SMELTING\",\"mcmmo_skill_SWORDS\",\"mcmmo_skill_TAMING\",\"mcmmo_skill_TRIDENTS\",\"mcmmo_skill_UNARMED\",\"mcmmo_skill_WOODCUTTING\",\"chemrank\",\"policerank\",\"donorrank\",\"goldrank\",\"current_month_votes\",\"plan_player_time_total_raw\",\"plan_player_time_month_raw\",\"plan_player_time_week_raw\",\"plan_player_time_day_raw\",\"plan_player_time_afk_raw\",\"plan_player_latest_session_length_raw\",\"plan_player_sessions_count\",\"plan_player_relativePlaytime_totalmonth\",\"plan_player_relativePlaytime_weekmonth\",\"plan_player_relativePlaytime_dayweek\",\"balance_change\",\"lw_rev_total_change\",\"lw_rev_phase_change\",\"leaderboard_position_chems_all_change\",\"leaderboard_position_chems_week_change\",\"leaderboard_position_police_all_change\",\"leaderboard_position_police_week_change\",\"chemrank_change\",\"policerank_change\",\"donorrank_change\",\"goldrank_change\"]\n",
    "\n",
    "# Create preprocessing transformers for encoding and scaling features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categoricalFeatures),\n",
    "        (\"num\", StandardScaler(), numericalFeatures)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define LogReg pipeline\n",
    "MCA_Pipeline_LogReg = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "# Define XGBoost pipeline\n",
    "MCA_Pipeline_XGB = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", num_class=3, verbosity=0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4c813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8313253012048193\n",
      "Confusion Matrix:\n",
      "[[ 0 13]\n",
      " [ 1 69]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.84      0.99      0.91        70\n",
      "\n",
      "    accuracy                           0.83        83\n",
      "   macro avg       0.42      0.49      0.45        83\n",
      "weighted avg       0.71      0.83      0.77        83\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrishinkson/Programming/Personal Projects/MCLabs/McLabsChurnAnalyzer/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LOGREG MODEL TRAINING AND TESTING\n",
    "'''\n",
    "# Train and test LogReg pipeline\n",
    "MCA_Pipeline_LogReg.fit(MCA_Features_Train, MCA_Target_Train)\n",
    "MCA_Target_Pred = MCA_Pipeline_LogReg.predict(MCA_Features_Test)\n",
    "print(f\"Accuracy: {accuracy_score(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(MCA_Target_Test, MCA_Target_Pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c379aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__scale_pos_weight': 1, 'model__subsample': 0.8}\n",
      "Best Cross-Validation Score: 0.8598135198135198\n",
      "Accuracy: 0.8313253012048193\n",
      "Confusion Matrix:\n",
      "[[ 2 11]\n",
      " [ 3 67]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.15      0.22        13\n",
      "           2       0.86      0.96      0.91        70\n",
      "\n",
      "    accuracy                           0.83        83\n",
      "   macro avg       0.63      0.56      0.56        83\n",
      "weighted avg       0.79      0.83      0.80        83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "XGBOOST MODEL TRAINING AND TESTING\n",
    "\n",
    "This cell will use a XGBoost pipeline and implement auto hyperparameter tuning to optimize the model's performance.\n",
    "'''\n",
    "\n",
    "# Map of hyperparameters and possible values to try tuning XGBoost with\n",
    "hyperParameterMap = {\n",
    "    \"model__n_estimators\": [100, 200, 400],      # boosting rounds\n",
    "    \"model__max_depth\": [3, 5, 7],               # tree depth\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.3],    # step size shrinkage\n",
    "    \"model__subsample\": [0.8, 1.0],              # row sampling\n",
    "    \"model__colsample_bytree\": [0.8, 1.0],       # feature sampling\n",
    "    \"model__scale_pos_weight\": [1, 2, 5]         # helps with class imbalance\n",
    "}\n",
    "\n",
    "# Grid search for best hyperparameters (5-fold CV)\n",
    "MCA_Pipeline_GridSearch_XGB = GridSearchCV(\n",
    "    estimator=MCA_Pipeline_XGB,\n",
    "    param_grid=hyperParameterMap,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train and test XGBoost pipeline\n",
    "MCA_Pipeline_GridSearch_XGB.fit(MCA_Features_Train, MCA_Target_Train)\n",
    "MCA_Target_Pred = MCA_Pipeline_GridSearch_XGB.predict(MCA_Features_Test)\n",
    "print(f\"Best Parameters: {MCA_Pipeline_GridSearch_XGB.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {MCA_Pipeline_GridSearch_XGB.best_score_}\")\n",
    "print(f\"Accuracy: {accuracy_score(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(MCA_Target_Test, MCA_Target_Pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebca9297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model-internals/MCA_Pipeline_GridSearch_XGB.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PIPELINE SAVING\n",
    "\n",
    "This section saves the entire machine learning pipeline to a file for future use.\n",
    "'''\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(MCA_LabelEncoder, \"../model-internals/MCA_LabelEncoder.pkl\")\n",
    "\n",
    "# Save the entire LogReg pipeline\n",
    "joblib.dump(MCA_Pipeline_LogReg, \"../model-internals/MCA_Pipeline_LogReg.pkl\")\n",
    "\n",
    "# Save the entire XGBoost pipeline\n",
    "joblib.dump(MCA_Pipeline_GridSearch_XGB, \"../model-internals/MCA_Pipeline_GridSearch_XGB.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
