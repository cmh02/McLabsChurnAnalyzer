{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281761f9",
   "metadata": {},
   "source": [
    "# MCLabs Churn Analyzer - Model Creation\n",
    "\n",
    "This Jupyter Notebook will create a ML model, train it on our training data, then offer a simple test analysis using test data.\n",
    "\n",
    "Note that this notebook converts the previous target encoding to a new encoding:\n",
    "- Not Active (Previously 0) -> Dropped\n",
    "- Recovered (Previously 1) -> 0\n",
    "- Churned (Previously 2) -> 1\n",
    "- Active (Previously 3) -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91861cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODULE/PACKAGE IMPORTS\n",
    "'''\n",
    "\n",
    "# System\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Custom Modules\n",
    "from mcalib import McaDataUtils, McaDataPrepare, McaFeaturePipeline, McaTargetPipeline\n",
    "\n",
    "# Pipelining\n",
    "import joblib\n",
    "\n",
    "# Output/Display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12bec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building master dataframe from 13 total timestamps!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Windows: 100%|██████████| 11/11 [00:02<00:00,  4.61window/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master DataFrame Shape: (6329, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "PRE-MODEL DATA PIPELINE\n",
    "'''\n",
    "\n",
    "def buildPipelineWindow(timestamp1: str, timestamp2: str, timestamp3: str) -> pd.DataFrame:\n",
    "\t# Load three data files for model training\n",
    "\tdf_t1 = McaDataUtils.getDfForTimestamp(timestamp=timestamp1)\n",
    "\tdf_t2 = McaDataUtils.getDfForTimestamp(timestamp=timestamp2)\n",
    "\tdf_t3 = McaDataUtils.getDfForTimestamp(timestamp=timestamp3)\n",
    "\n",
    "\t# Prepare all datasets\n",
    "\tdf_t1 = McaDataPrepare.prepareData(df=df_t1, dfTimestamp=float(timestamp1))\n",
    "\tdf_t2 = McaDataPrepare.prepareData(df=df_t2, dfTimestamp=float(timestamp2))\n",
    "\tdf_t3 = McaDataPrepare.prepareData(df=df_t3, dfTimestamp=float(timestamp3))\n",
    "\n",
    "\t# Perform feature engineering between the first two timestamps\n",
    "\tdf = McaFeaturePipeline.combineData(currentDf=df_t2, previousDf=df_t1)\n",
    "\n",
    "\t# Perform target engineering between the last two timestamps\n",
    "\tdf = McaTargetPipeline.buildTarget(currentDf=df, futureDf=df_t3, onlyReturnTarget=False)\n",
    "\n",
    "\t# Drop UUID's before model\n",
    "\tdf = McaDataUtils.clearUUIDs(df=df)\n",
    "\n",
    "\t# For now, drop rows where target is 0 (completely inactive)\n",
    "\tdf = df[df[\"churn\"] != 0].reset_index(drop=True)\n",
    "\n",
    "\treturn df\n",
    "\n",
    "# Make a dataframe for holding all data\n",
    "masterDf = pd.DataFrame()\n",
    "\n",
    "# Get all of the timestamps available\n",
    "dataDirectory = Path(\"../data/gatheringoutput/\")\n",
    "timestamps = [path.name for path in dataDirectory.iterdir() if path.is_dir()]\n",
    "print(f\"Building master dataframe from {len(timestamps)} total timestamps!\")\n",
    "\n",
    "# Append each window's data to master dataframe\n",
    "for window in tqdm(iterable=[timestamps[i:i+3] for i in range(len(timestamps) - 2)], desc=\"Processing Windows\", unit=\"window\"):\n",
    "\ttestDf = buildPipelineWindow(timestamp1=window[0], timestamp2=window[1], timestamp3=window[2])\n",
    "\tmasterDf = pd.concat([masterDf, testDf], ignore_index=True)\n",
    "\n",
    "# Print master dataframe shape\n",
    "print(f\"Master DataFrame Shape: {masterDf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fe80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PIPELINE CREATION\n",
    "\n",
    "This section will create a pipeline for loading the data, splitting the data, scaling the data, and training the model.\n",
    "'''\n",
    "\n",
    "# Separate features from target\n",
    "MCA_Features = masterDf.drop(columns=[\"churn\"])\n",
    "MCA_Target = masterDf[\"churn\"]\n",
    "\n",
    "# Transform target to be 0, 1, 2 instead of 1, 2, 3\n",
    "MCA_LabelEncoder = LabelEncoder()\n",
    "MCA_Target =  MCA_LabelEncoder.fit_transform(MCA_Target)\n",
    "\n",
    "# Split the data\n",
    "MCA_Features_Train, MCA_Features_Test, MCA_Target_Train, MCA_Target_Test = train_test_split(MCA_Features, MCA_Target, test_size=0.2, stratify=MCA_Target, random_state=2002)\n",
    "\n",
    "# Identify which features are categorical\n",
    "categoricalFeatures = [\"plan_player_favorite_server\"]\n",
    "\n",
    "# Identify which features are numerical (note we do not include the last seen time here)\n",
    "numericalFeatures = [\"balance\",\"lw_rev_total\",\"lw_rev_phase\",\"leaderboard_position_chems_all\",\"leaderboard_position_chems_week\",\"leaderboard_position_police_all\",\"leaderboard_position_police_week\",\"mcmmo_power_level\",\"mcmmo_skill_ACROBATICS\",\"mcmmo_skill_ALCHEMY\",\"mcmmo_skill_ARCHERY\",\"mcmmo_skill_AXES\",\"mcmmo_skill_CROSSBOWS\",\"mcmmo_skill_EXCAVATION\",\"mcmmo_skill_FISHING\",\"mcmmo_skill_HERBALISM\",\"mcmmo_skill_MACES\",\"mcmmo_skill_MINING\",\"mcmmo_skill_REPAIR\",\"mcmmo_skill_SALVAGE\",\"mcmmo_skill_SMELTING\",\"mcmmo_skill_SWORDS\",\"mcmmo_skill_TAMING\",\"mcmmo_skill_TRIDENTS\",\"mcmmo_skill_UNARMED\",\"mcmmo_skill_WOODCUTTING\",\"chemrank\",\"policerank\",\"donorrank\",\"goldrank\",\"current_month_votes\",\"plan_player_time_total_raw\",\"plan_player_time_month_raw\",\"plan_player_time_week_raw\",\"plan_player_time_day_raw\",\"plan_player_time_afk_raw\",\"plan_player_latest_session_length_raw\",\"plan_player_sessions_count\",\"plan_player_relativePlaytime_totalmonth\",\"plan_player_relativePlaytime_weekmonth\",\"plan_player_relativePlaytime_dayweek\",\"balance_change\",\"lw_rev_total_change\",\"lw_rev_phase_change\",\"leaderboard_position_chems_all_change\",\"leaderboard_position_chems_week_change\",\"leaderboard_position_police_all_change\",\"leaderboard_position_police_week_change\",\"chemrank_change\",\"policerank_change\",\"donorrank_change\",\"goldrank_change\"]\n",
    "\n",
    "# Create preprocessing transformers for encoding and scaling features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categoricalFeatures),\n",
    "        (\"num\", StandardScaler(), numericalFeatures)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a debugging class to see data info at each step\n",
    "class DebugStep:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(f\"{self.name}: {type(X)}, shape={getattr(X, 'shape', None)}\")\n",
    "        return X\n",
    "\n",
    "# Define LogReg pipeline\n",
    "MCA_Pipeline_LogReg = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "# Define XGBoost pipeline\n",
    "MCA_Pipeline_XGB = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", num_class=3, verbosity=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f7a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pre shape: (5063, 57)\n",
      "NaN count (train): 0\n",
      "Inf count (train): 0\n",
      "X_test_pre shape: (1266, 57)\n",
      "NaN count (test): 0\n",
      "Inf count (test): 0\n"
     ]
    }
   ],
   "source": [
    "# Transform outside the pipeline for inspection\n",
    "X_train_pre = preprocessor.fit_transform(MCA_Features_Train)\n",
    "X_test_pre = preprocessor.transform(MCA_Features_Test)\n",
    "\n",
    "print(\"X_train_pre shape:\", X_train_pre.shape)\n",
    "print(\"NaN count (train):\", np.isnan(X_train_pre).sum())\n",
    "print(\"Inf count (train):\", np.isinf(X_train_pre).sum())\n",
    "\n",
    "print(\"X_test_pre shape:\", X_test_pre.shape)\n",
    "print(\"NaN count (test):\", np.isnan(X_test_pre).sum())\n",
    "print(\"Inf count (test):\", np.isinf(X_test_pre).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b4c813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6169036334913112\n",
      "Confusion Matrix:\n",
      "[[358  12  13]\n",
      " [226 232  52]\n",
      " [ 83  99 191]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.93      0.68       383\n",
      "           1       0.68      0.45      0.54       510\n",
      "           2       0.75      0.51      0.61       373\n",
      "\n",
      "    accuracy                           0.62      1266\n",
      "   macro avg       0.65      0.63      0.61      1266\n",
      "weighted avg       0.65      0.62      0.60      1266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LOGREG MODEL TRAINING AND TESTING\n",
    "'''\n",
    "# Train and test LogReg pipeline\n",
    "MCA_Pipeline_LogReg.fit(MCA_Features_Train, MCA_Target_Train)\n",
    "MCA_Target_Pred = MCA_Pipeline_LogReg.predict(MCA_Features_Test)\n",
    "print(f\"Accuracy: {accuracy_score(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(MCA_Target_Test, MCA_Target_Pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c379aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.01, 'model__max_depth': 7, 'model__n_estimators': 100, 'model__scale_pos_weight': 1, 'model__subsample': 1.0}\n",
      "Best Cross-Validation Score: 0.7191406966354389\n",
      "Accuracy: 0.7195892575039494\n",
      "Confusion Matrix:\n",
      "[[272  93  18]\n",
      " [ 31 408  71]\n",
      " [ 36 106 231]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75       383\n",
      "           1       0.67      0.80      0.73       510\n",
      "           2       0.72      0.62      0.67       373\n",
      "\n",
      "    accuracy                           0.72      1266\n",
      "   macro avg       0.73      0.71      0.72      1266\n",
      "weighted avg       0.73      0.72      0.72      1266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "XGBOOST MODEL TRAINING AND TESTING\n",
    "\n",
    "This cell will use a XGBoost pipeline and implement auto hyperparameter tuning to optimize the model's performance.\n",
    "'''\n",
    "\n",
    "# Map of hyperparameters and possible values to try tuning XGBoost with\n",
    "hyperParameterMap = {\n",
    "    \"model__n_estimators\": [100, 200, 400],      # boosting rounds\n",
    "    \"model__max_depth\": [3, 5, 7],               # tree depth\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.3],    # step size shrinkage\n",
    "    \"model__subsample\": [0.8, 1.0],              # row sampling\n",
    "    \"model__colsample_bytree\": [0.8, 1.0],       # feature sampling\n",
    "    \"model__scale_pos_weight\": [1, 2, 5]         # helps with class imbalance\n",
    "}\n",
    "\n",
    "# Grid search for best hyperparameters (5-fold CV)\n",
    "MCA_Pipeline_GridSearch_XGB = GridSearchCV(\n",
    "    estimator=MCA_Pipeline_XGB,\n",
    "    param_grid=hyperParameterMap,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train and test XGBoost pipeline\n",
    "MCA_Pipeline_GridSearch_XGB.fit(MCA_Features_Train, MCA_Target_Train)\n",
    "MCA_Target_Pred = MCA_Pipeline_GridSearch_XGB.predict(MCA_Features_Test)\n",
    "print(f\"Best Parameters: {MCA_Pipeline_GridSearch_XGB.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score: {MCA_Pipeline_GridSearch_XGB.best_score_}\")\n",
    "print(f\"Accuracy: {accuracy_score(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(MCA_Target_Test, MCA_Target_Pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(MCA_Target_Test, MCA_Target_Pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebca9297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model-internals/MCA_Pipeline_GridSearch_XGB.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PIPELINE SAVING\n",
    "\n",
    "This section saves the entire machine learning pipeline to a file for future use.\n",
    "'''\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(MCA_LabelEncoder, \"../model-internals/MCA_LabelEncoder.pkl\")\n",
    "\n",
    "# Save the entire LogReg pipeline\n",
    "joblib.dump(MCA_Pipeline_LogReg, \"../model-internals/MCA_Pipeline_LogReg.pkl\")\n",
    "\n",
    "# Save the entire XGBoost pipeline\n",
    "joblib.dump(MCA_Pipeline_GridSearch_XGB, \"../model-internals/MCA_Pipeline_GridSearch_XGB.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
