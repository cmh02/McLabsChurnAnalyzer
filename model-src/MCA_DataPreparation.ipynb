{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375d8291",
   "metadata": {},
   "source": [
    "# MCLabs Churn Analyzer - Data Preparation\n",
    "Author: @cmh02\n",
    "\n",
    "This Jupyter Notebook will be used for general data preparation for the model. As all of our data is coming from separate sources, we first need to combine our data into a singular source. We then will record some values so that we can later derive more features. We will then pre-process the data to address a variety of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a10271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODULE/PACKAGE IMPORTS\n",
    "'''\n",
    "\n",
    "# System\n",
    "import os\n",
    "import hashlib\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Output/Display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de0d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ENVIRONMENT VARIABLES\n",
    "'''\n",
    "\n",
    "# Load environment file using python-dotenv\n",
    "load_dotenv(dotenv_path=\"../env/.env\")\n",
    "\n",
    "# Load environmental variables\n",
    "MCA_PEPPERKEY = os.getenv(\"MCA_PEPPERKEY\")\n",
    "\n",
    "# Ensure environmental variables are set\n",
    "if not MCA_PEPPERKEY:\n",
    "    raise ValueError(\"Missing required environment variable: MCA_PEPPERKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ebeb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anonymizing Data Files: 100%|██████████| 7/7 [00:00<00:00, 54.48file/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATA ANONYMIZATION\n",
    "\n",
    "To protect player privacy, the first portion of our data preparation is anonymizing our data. \n",
    "We will simply take the datafiles in gatheringoutput and replace the UUID field with a hashed\n",
    "version of itself. Then we can save a private version, including these hashes, for our usage,\n",
    "along with a public version, with no UUID's or hashes, for external analysis.\n",
    "'''\n",
    "\n",
    "# Define input and output folder paths\n",
    "folderPath_gatheringoutput = \"../data/gatheringoutput/\"\n",
    "folderPath_anonoutput_public = \"../data/anonoutput/public/\"\n",
    "folderPath_anonoutput_private = \"../data/anonoutput/private/\"\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "if not os.path.exists(folderPath_anonoutput_public):\n",
    "    os.makedirs(folderPath_anonoutput_public, exist_ok=True)\n",
    "if not os.path.exists(folderPath_anonoutput_private):\n",
    "    os.makedirs(folderPath_anonoutput_private, exist_ok=True)\n",
    "\n",
    "# Get the names of all gatheringoutput files using glob\n",
    "gatheringOutputFiles = glob(os.path.join(folderPath_gatheringoutput, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "# Iterate through the files\n",
    "for filePath in tqdm(iterable=gatheringOutputFiles, desc=\"Anonymizing Data Files\", unit=\"file\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filePath)\n",
    "\n",
    "    # Anonymize the UUID column (UUID -> hash(PEPPER + UUID))\n",
    "    df['UUID'] = [hashlib.sha256(f\"{MCA_PEPPERKEY}:{uuid}\".encode()).hexdigest() for uuid in df['UUID']]\n",
    "\n",
    "    # Get relative path for gatheringoutput file location\n",
    "    dataRelativeFilePath = os.path.relpath(filePath, folderPath_gatheringoutput)\n",
    "    \n",
    "\t# Create private output path and save dataframe to path\n",
    "    outputFilePath = os.path.join(folderPath_anonoutput_private, dataRelativeFilePath)\n",
    "    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "    df.to_csv(outputFilePath, index=False)\n",
    "    \n",
    "\t# Drop the UUID column\n",
    "    df.drop(columns=['UUID'], inplace=True)\n",
    "    \n",
    "\t# Create public output path and save dataframe to path\n",
    "    outputFilePath = os.path.join(folderPath_anonoutput_public, dataRelativeFilePath)\n",
    "    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "    df.to_csv(outputFilePath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee95335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining Data Files: 100%|██████████| 7/7 [00:00<00:00, 111.39file/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATA COMBINING\n",
    "\n",
    "The next step in the data preparation process is combining all of our data from the various\n",
    "data sources into a single dataset. We will take all data files located in the `anonoutput`\n",
    "data directory and join them based on the UUID hash. All of the data will then be saved\n",
    "in a single output file in the `combined` directory.\n",
    "'''\n",
    "\n",
    "# Define input and output folder paths\n",
    "folderPath_anonoutput_private = \"../data/anonoutput/private/\"\n",
    "folderPath_combined_public = \"../data/combined/public/\"\n",
    "folderPath_combined_private = \"../data/combined/private/\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(folderPath_combined_public):\n",
    "    os.makedirs(folderPath_combined_public, exist_ok=True)\n",
    "if not os.path.exists(folderPath_combined_private):\n",
    "    os.makedirs(folderPath_combined_private, exist_ok=True)\n",
    "\n",
    "# Get the names of all anonoutput files using glob\n",
    "anonOutputFiles = glob(os.path.join(folderPath_anonoutput_private, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "# Initialize an empty DataFrame to hold combined data\n",
    "combinedDataFrame = pd.DataFrame(columns=[\"UUID\"])\n",
    "\n",
    "# Iterate through the files\n",
    "for filePath in tqdm(iterable=anonOutputFiles, desc=\"Combining Data Files\", unit=\"file\"):\n",
    "\t# Read the CSV file\n",
    "\tdf = pd.read_csv(filePath)\n",
    "\n",
    "\t# Merge the DataFrame with the combinedData DataFrame\n",
    "\tcombinedDataFrame = pd.merge(left=combinedDataFrame, right=df, on=\"UUID\", how=\"outer\")\n",
    "\n",
    "# Get relative path for output file\n",
    "dataRelativeFilePath = os.path.relpath(filePath, folderPath_anonoutput_private)\n",
    "directoryPath = os.path.dirname(dataRelativeFilePath)\n",
    "dataRelativeFilePath = os.path.join(directoryPath, \"combined.csv\")\n",
    "\n",
    "# Create private output path and save combined data\n",
    "outputFilePath = os.path.join(folderPath_combined_private, dataRelativeFilePath)\n",
    "os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "combinedDataFrame.to_csv(outputFilePath, index=False)\n",
    "\n",
    "# Drop the UUID column\n",
    "combinedDataFrame.drop(columns=[\"UUID\"], inplace=True)\n",
    "\n",
    "# Create public output path and save combined data\n",
    "outputFilePath = os.path.join(folderPath_combined_public, dataRelativeFilePath)\n",
    "os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "combinedDataFrame.to_csv(outputFilePath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
