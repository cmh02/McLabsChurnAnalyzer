{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375d8291",
   "metadata": {},
   "source": [
    "# MCLabs Churn Analyzer - Data Preparation\n",
    "Author: @cmh02\n",
    "\n",
    "This Jupyter Notebook will be used for general data preparation for the model in three main steps:\n",
    "1) Anonymizing the data\n",
    "2) Combining the data\n",
    "3) Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a10271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODULE/PACKAGE IMPORTS\n",
    "'''\n",
    "\n",
    "# System\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Output/Display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ENVIRONMENT VARIABLES\n",
    "'''\n",
    "\n",
    "# Load environment file using python-dotenv\n",
    "load_dotenv(dotenv_path=\"../env/.env\")\n",
    "\n",
    "# Load environmental variables\n",
    "MCA_PEPPERKEY = os.getenv(\"MCA_PEPPERKEY\")\n",
    "\n",
    "# Ensure environmental variables are set\n",
    "if not MCA_PEPPERKEY:\n",
    "\traise ValueError(\"Missing required environment variable: MCA_PEPPERKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ebeb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anonymizing Data Files: 100%|██████████| 7/7 [00:00<00:00, 54.48file/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATA ANONYMIZATION\n",
    "\n",
    "To protect player privacy, the first portion of our data preparation is anonymizing our data. \n",
    "We will simply take the datafiles in gatheringoutput and replace the UUID field with a hashed\n",
    "version of itself. Then we can save a private version, including these hashes, for our usage,\n",
    "along with a public version, with no UUID's or hashes, for external analysis.\n",
    "'''\n",
    "\n",
    "# Define input and output folder paths\n",
    "folderPath_gatheringoutput = \"../data/gatheringoutput/\"\n",
    "folderPath_anonoutput_public = \"../data/anonoutput/public/\"\n",
    "folderPath_anonoutput_private = \"../data/anonoutput/private/\"\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "if not os.path.exists(folderPath_anonoutput_public):\n",
    "\tos.makedirs(folderPath_anonoutput_public, exist_ok=True)\n",
    "if not os.path.exists(folderPath_anonoutput_private):\n",
    "\tos.makedirs(folderPath_anonoutput_private, exist_ok=True)\n",
    "\n",
    "# Get the names of all gatheringoutput files using glob\n",
    "gatheringOutputFiles = glob(os.path.join(folderPath_gatheringoutput, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "# Iterate through the files\n",
    "for filePath in tqdm(iterable=gatheringOutputFiles, desc=\"Anonymizing Data Files\", unit=\"file\"):\n",
    "\t# Read the CSV file\n",
    "\tdf = pd.read_csv(filePath)\n",
    "\n",
    "\t# Anonymize the UUID column (UUID -> hash(PEPPER + UUID))\n",
    "\tdf['UUID'] = [hashlib.sha256(f\"{MCA_PEPPERKEY}:{uuid}\".encode()).hexdigest() for uuid in df['UUID']]\n",
    "\n",
    "\t# Get relative path for gatheringoutput file location\n",
    "\tdataRelativeFilePath = os.path.relpath(filePath, folderPath_gatheringoutput)\n",
    "\t\n",
    "\t# Create private output path and save dataframe to path\n",
    "\toutputFilePath = os.path.join(folderPath_anonoutput_private, dataRelativeFilePath)\n",
    "\tos.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "\tdf.to_csv(outputFilePath, index=False)\n",
    "\t\n",
    "\t# Drop the UUID column\n",
    "\tdf.drop(columns=['UUID'], inplace=True)\n",
    "\t\n",
    "\t# Create public output path and save dataframe to path\n",
    "\toutputFilePath = os.path.join(folderPath_anonoutput_public, dataRelativeFilePath)\n",
    "\tos.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "\tdf.to_csv(outputFilePath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee95335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining Data Files: 100%|██████████| 7/7 [00:00<00:00, 111.39file/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATA COMBINING\n",
    "\n",
    "The next step in the data preparation process is combining all of our data from the various\n",
    "data sources into a single dataset. We will take all data files located in the `anonoutput`\n",
    "data directory and join them based on the UUID hash. All of the data will then be saved\n",
    "in a single output file in the `combined` directory.\n",
    "'''\n",
    "\n",
    "# Define input and output folder paths\n",
    "folderPath_anonoutput_private = \"../data/anonoutput/private/\"\n",
    "folderPath_combined_public = \"../data/combined/public/\"\n",
    "folderPath_combined_private = \"../data/combined/private/\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(folderPath_combined_public):\n",
    "\tos.makedirs(folderPath_combined_public, exist_ok=True)\n",
    "if not os.path.exists(folderPath_combined_private):\n",
    "\tos.makedirs(folderPath_combined_private, exist_ok=True)\n",
    "\n",
    "# Get the names of all anonoutput files using glob\n",
    "anonOutputFiles = glob(os.path.join(folderPath_anonoutput_private, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "# Initialize an empty DataFrame to hold combined data\n",
    "combinedDataFrame = pd.DataFrame(columns=[\"UUID\"])\n",
    "\n",
    "# Iterate through the files\n",
    "for filePath in tqdm(iterable=anonOutputFiles, desc=\"Combining Data Files\", unit=\"file\"):\n",
    "\t# Read the CSV file\n",
    "\tdf = pd.read_csv(filePath)\n",
    "\n",
    "\t# Merge the DataFrame with the combinedData DataFrame\n",
    "\tcombinedDataFrame = pd.merge(left=combinedDataFrame, right=df, on=\"UUID\", how=\"outer\")\n",
    "\n",
    "# Get relative path for output file\n",
    "dataRelativeFilePath = os.path.relpath(filePath, folderPath_anonoutput_private)\n",
    "directoryPath = os.path.dirname(dataRelativeFilePath)\n",
    "dataRelativeFilePath = os.path.join(directoryPath, \"combined.csv\")\n",
    "\n",
    "# Create private output path and save combined data\n",
    "outputFilePath = os.path.join(folderPath_combined_private, dataRelativeFilePath)\n",
    "os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "combinedDataFrame.to_csv(outputFilePath, index=False)\n",
    "\n",
    "# Drop the UUID column\n",
    "combinedDataFrame.drop(columns=[\"UUID\"], inplace=True)\n",
    "\n",
    "# Create public output path and save combined data\n",
    "outputFilePath = os.path.join(folderPath_combined_public, dataRelativeFilePath)\n",
    "os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "combinedDataFrame.to_csv(outputFilePath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec7cbf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Data Files: 100%|██████████| 1/1 [00:00<00:00, 10.16file/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DATA CLEANING\n",
    "\n",
    "This cell will clean the data to accomplish a variety of tasks:\n",
    "- For any missing values, the cell will be replaced with NULL values\n",
    "- All <none> values will be turned into formal NULL values\n",
    "- Players missing a last-seen date will be dropped\n",
    "- Players missing other features will be filled with default values\n",
    "'''\n",
    "\n",
    "# Utility function for converting between Plan's date syntax and seconds-since-date\n",
    "def planDateToSecondsSince(planDateString: str, untilUnixTimeStamp: int) -> int:\n",
    "\t# Convert the unix timestamp into a datetime object\n",
    "\tuntilDateTime = datetime.fromtimestamp(untilUnixTimeStamp)\n",
    "\n",
    "\t# Check if the plan date is in weekday formatting or date formatting\n",
    "\tif re.match(r\"^\\S+ \\S+ \\S+ \\S+:\\S+$\", planDateString):\n",
    "\t\t\n",
    "\t\t# If we already have date formatting, just parse the string\n",
    "\t\tplanDateTime = datetime.strptime(planDateString, \"%b %d %Y %H:%M\")\n",
    "\n",
    "\telse:\n",
    "\n",
    "\t\t# If we have relative day-of-week formatting, then we need to figure out which day it is\n",
    "\t\tplanDateStringSplit = planDateString.split(\" \")\n",
    "\t\tplanDayOfWeek = planDateStringSplit[0]\n",
    "\t\tplanTimeOfDay = planDateStringSplit[1]\n",
    "\n",
    "\t\t# If the day is \"Today\", then it is really the day before recording\n",
    "\t\tif planDayOfWeek == \"Today\":\n",
    "\t\t\tplanDateTime = untilDateTime - timedelta(days=1)\n",
    "\t\t\n",
    "\t\t# If the day is \"Yesterday\", then it is 2 days before\n",
    "\t\telif planDayOfWeek == \"Yesterday\":\n",
    "\t\t\tplanDateTime = untilDateTime - timedelta(days=2)\n",
    "\n",
    "\t\t# If the day is shown as a day-of-the-week, then calc difference + 1\n",
    "\t\telif planDayOfWeek in [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]:\n",
    "\n",
    "\t\t\t# Get week of the day as datetime\n",
    "\t\t\tplanDateTime = datetime.strptime(planDayOfWeek, \"%A\")\n",
    "\n",
    "\t\t\t# Calculate difference in days then add 1 for offset\n",
    "\t\t\trelativeDaysPassed = (untilDateTime.weekday() - planDateTime.weekday()) % 7\n",
    "\t\t\tplanDateTime = untilDateTime - timedelta(days=relativeDaysPassed + 1)\n",
    "\n",
    "\t\t# Detect incorrect formatting\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Unrecognized plan date format: {planDateString}\")\n",
    "\n",
    "\t\t# Set the time\n",
    "\t\tplanDateTime = planDateTime.replace(hour=int(planTimeOfDay.split(\":\")[0]), minute=int(planTimeOfDay.split(\":\")[1]))\n",
    "\n",
    "\t# Return difference of the two dates in seconds\n",
    "\treturn abs(int((planDateTime - untilDateTime).total_seconds()))\n",
    "\n",
    "# Define input and output folder paths\n",
    "folderPath_combined_private = \"../data/combined/private/\"\n",
    "folderPath_cleaned_public = \"../data/cleaned/public/\"\n",
    "folderPath_cleaned_private = \"../data/cleaned/private/\"\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "if not os.path.exists(folderPath_cleaned_public):\n",
    "\tos.makedirs(folderPath_cleaned_public, exist_ok=True)\n",
    "if not os.path.exists(folderPath_cleaned_private):\n",
    "\tos.makedirs(folderPath_cleaned_private, exist_ok=True)\n",
    "\t\n",
    "# Get the names of all combined files using glob\n",
    "combinedOutputFiles = glob(os.path.join(folderPath_combined_private, \"**\", \"*.csv\"), recursive=True)\n",
    "\n",
    "# Clean each file\n",
    "for filePath in tqdm(iterable=combinedOutputFiles, desc=\"Cleaning Data Files\", unit=\"file\"):\n",
    "\n",
    "\t# Load the file into a DataFrame\n",
    "\tdf = pd.read_csv(filePath)\n",
    "\n",
    "\t# Replace <none> with NULL\n",
    "\tdf.replace(\"<none>\", pd.NA, inplace=True)\n",
    "\t\n",
    "\t# Replace \"-\" with NULL\n",
    "\tdf.replace(\"-\", pd.NA, inplace=True)\n",
    "\n",
    "\t# Drop players missing a last-seen date\n",
    "\tdf.dropna(subset=[\" plan_player_lastseen\"], inplace=True)\n",
    "\n",
    "\t# Fill missing values for other features\n",
    "\tdf.fillna({\n",
    "\t\t\" mcmmo_power_level\": 0,\n",
    "\t\t\" mcmmo_skill_ACROBATICS\": 0,\n",
    "\t\t\" mcmmo_skill_ALCHEMY\": 0,\n",
    "\t\t\" mcmmo_skill_ARCHERY\": 0,\n",
    "\t\t\" mcmmo_skill_AXES\": 0,\n",
    "\t\t\" mcmmo_skill_CROSSBOWS\": 0,\n",
    "\t\t\" mcmmo_skill_EXCAVATION\": 0,\n",
    "\t\t\" mcmmo_skill_FISHING\": 0,\n",
    "\t\t\" mcmmo_skill_HERBALISM\": 0,\n",
    "\t\t\" mcmmo_skill_MACES\": 0,\n",
    "\t\t\" mcmmo_skill_MINING\": 0,\n",
    "\t\t\" mcmmo_skill_REPAIR\": 0,\n",
    "\t\t\" mcmmo_skill_SALVAGE\": 0,\n",
    "\t\t\" mcmmo_skill_SMELTING\": 0,\n",
    "\t\t\" mcmmo_skill_SWORDS\": 0,\n",
    "\t\t\" mcmmo_skill_TAMING\": 0,\n",
    "\t\t\" mcmmo_skill_TRIDENTS\": 0,\n",
    "\t\t\" mcmmo_skill_UNARMED\": 0,\n",
    "\t\t\" mcmmo_skill_WOODCUTTING\": 0,\n",
    "\t\t\" lw_rev_total\": 0,\n",
    "\t\t\" lw_rev_phase\": 0,\n",
    "\t\t\" chemrank\": 0,\n",
    "\t\t\" policerank\": 0,\n",
    "\t\t\" donorrank\": 0,\n",
    "\t\t\" goldrank\": 0,\n",
    "\t\t\" current_month_votes\": 0,\n",
    "\t\t\" plan_player_time_total_raw\": 0,\n",
    "\t\t\" plan_player_time_month_raw\": 0,\n",
    "\t\t\" plan_player_time_week_raw\": 0,\n",
    "\t\t\" plan_player_time_day_raw\": 0,\n",
    "\t\t\" plan_player_time_afk_raw\": 0,\n",
    "\t\t\" plan_player_latest_session_length_raw\": 0,\n",
    "\t\t\" plan_player_favorite_server\": \"Spawn\",\n",
    "\t\t\" plan_player_sessions_count\": 1,\n",
    "\t\t\" leaderboard_position_chems_all\": 0,\n",
    "\t\t\" leaderboard_position_chems_week\": 0,\n",
    "\t\t\" leaderboard_position_police_all\": 0,\n",
    "\t\t\" leaderboard_position_police_week\": 0,\n",
    "\t\t\" balance\": 0\n",
    "\t}, inplace=True)\n",
    "\t \n",
    "\t# Convert last seen times into seconds since last seen\n",
    "\trecordingTimestamp = int(os.path.basename(os.path.dirname(filePath)))\n",
    "\tdf[\" plan_player_lastseen\"] = df[\" plan_player_lastseen\"].apply(lambda x: planDateToSecondsSince(x, recordingTimestamp))\n",
    "\n",
    "\t# Get relative path for output file\n",
    "\tdataRelativeFilePath = os.path.relpath(filePath, folderPath_combined_private)\n",
    "\tdirectoryPath = os.path.dirname(dataRelativeFilePath)\n",
    "\tdataRelativeFilePath = os.path.join(directoryPath, \"cleaned.csv\")\n",
    "\n",
    "\t# Create private output path and save combined data\n",
    "\toutputFilePath = os.path.join(folderPath_cleaned_private, dataRelativeFilePath)\n",
    "\tos.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "\tdf.to_csv(outputFilePath, index=False)\n",
    "\n",
    "\t# Drop the UUID column\n",
    "\tdf.drop(columns=[\"UUID\"], inplace=True)\n",
    "\n",
    "\t# Create public output path and save combined data\n",
    "\toutputFilePath = os.path.join(folderPath_cleaned_public, dataRelativeFilePath)\n",
    "\tos.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n",
    "\tdf.to_csv(outputFilePath, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
